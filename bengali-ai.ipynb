{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"/home/jupyter/BengaliAi/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tb-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations==0.0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute():\n",
    "    df = pd.read_csv(\"input/train.csv\")\n",
    "    print(df.head())\n",
    "    df.loc[:,\"kfold\"] = -1\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X = df.image_id.values\n",
    "    y = df[[\"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\"]].values\n",
    "    \n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "# #     for trn_,val_ in mskf.split(X,y):\n",
    "# #         print(\"TRAIN:\", trn_, \"Val:\", val_)\n",
    "    for fold, (trn_,val_) in enumerate(mskf.split(X,y)):\n",
    "        print(\"TRAIN:\", len(trn_), \"Val:\", len(val_))\n",
    "# #         df.loc[val_,\"kfold\"] = fold\n",
    "\n",
    "# #     print(df.kfold.value_counts())\n",
    "# #     df.to_csv(\"input/train_folds.csv\", index=False)\n",
    "compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import albumentations\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_pickels():\n",
    "    files = glob.glob(\"input/train_image_data_*.parquet\")\n",
    "    for f in files:\n",
    "        df = pd.read_parquet(f)\n",
    "        image_ids = df.image_id.values\n",
    "        df = df.drop(\"image_id\",axis=1)\n",
    "        image_array = df.values\n",
    "        for j,img_id in tqdm(enumerate(image_ids),total=len(image_ids)):\n",
    "            joblib.dump(image_array[j,:],f\"input/image_pickels/{img_id}.pkl\")\n",
    "create_image_pickels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDatasetTrain:\n",
    "    def __init__(self,folds,img_height,img_width,mean,std):\n",
    "        df = pd.read_csv(\"input/train_folds.csv\")\n",
    "        df = df.drop(\"grapheme\",axis=1)\n",
    "#         df = [[\"image_id\",\"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\", \"kfold\"]]\n",
    "        q = df.kfold.isin(folds)\n",
    "        df = df[q].reset_index(drop=True)\n",
    "        self.image_ids = df.image_id.values\n",
    "        self.grapheme_root = df.grapheme_root.values\n",
    "        self.vowel_diacritic = df.vowel_diacritic.values\n",
    "        self.consonant_diacritic = df.consonant_diacritic.values\n",
    "        \n",
    "        if(len(folds)==1):\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height,img_width,always_apply=True),\n",
    "                albumentations.Normalize(mean,std,always_apply=True)\n",
    "            ])\n",
    "        else:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height,img_width,always_apply=True),\n",
    "                albumentations.ShiftScaleRotate(shift_limit=0.0625,\n",
    "                                                scale_limit=0.1,\n",
    "                                                rotate_limit=5,\n",
    "                                                p=0.9),\n",
    "                albumentations.Normalize(mean,std,always_apply=True)\n",
    "            ])\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "#         print(self.image_ids[item])\n",
    "        image = joblib.load(f\"input/image_pickels/{self.image_ids[item]}.pkl\")\n",
    "        image = image.reshape(137,236).astype(float)\n",
    "        image = Image.fromarray(image).convert(\"RGB\")\n",
    "        \n",
    "        image = self.aug(image = np.array(image))[\"image\"]\n",
    "        image = np.transpose(image,(2,0,1)).astype(np.float32)\n",
    "        return{\n",
    "            'image': torch.tensor(image,dtype=torch.float),\n",
    "            'grapheme_root': torch.tensor(self.grapheme_root[item],dtype=torch.long),\n",
    "            'vowel_diacritic': torch.tensor(self.vowel_diacritic[item],dtype=torch.long),\n",
    "            'consonant_diacritic': torch.tensor(self.consonant_diacritic[item],dtype=torch.long)\n",
    "        }\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset = BengaliDatasetTrain(folds=[0,1],img_height=137, img_width=236,mean=(0.485,0.456,0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "img = datset[idx][\"image\"]\n",
    "print(datset[idx][\"grapheme_root\"])\n",
    "print(datset[idx][\"vowel_diacritic\"])\n",
    "print(datset[idx][\"consonant_diacritic\"])\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg,(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self,pretrained):\n",
    "        super(ResNet34,self).__init__()\n",
    "        if pretrained is True:\n",
    "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n",
    "        \n",
    "        self.l0 = nn.Linear(512,168)\n",
    "        self.l1 = nn.Linear(512,11)\n",
    "        self.l2 = nn.Linear(512,7)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        bs,_,_,_ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(bs,-1)\n",
    "        l0 = self.l0(x)\n",
    "        l1 = self.l1(x)\n",
    "        l2 = self.l2(x)\n",
    "        \n",
    "        return l0,l1,l2\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPATCHER = {\n",
    "    'resnet34': ResNet34\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL_DISPATCHER[\"resnet34\"](pretrained=False)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sh src/run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "# TRAINING_FOLDS_CSV = os.getenv(\"TRAINING_FOLDS_CSV\")\n",
    "# IMG_HEIGHT = os.environ.get(\"IMG_HEIGHT\")\n",
    "# IMG_WIDTH = os.environ.get(\"IMG_WIDTH\")\n",
    "# EPOCHS = os.environ.get(\"EPOCHS\")\n",
    "\n",
    "# TRAIN_BATCH_SIZE = os.environ.get(\"TRAIN_BATCH_SIZE\")\n",
    "# TEST_BATCH_SIZE = os.environ.get(\"TEST_BATCH_SIZE\")\n",
    "\n",
    "# MODEL_MEAN = os.environ.get(\"MODEL_MEAN\")\n",
    "# MODEL_STD = os.environ.get(\"MODEL_STD\")\n",
    "\n",
    "# TRAINING_FOLDS = os.environ.get(\"TRAINING_FOLDS\")\n",
    "# VALIDATION_FOLDS =os.environ.get(\"VALIDATION_FOLDS\")\n",
    "# BASE_MODEL = os.environ.get(\"BASE_MODEL\")\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "IMG_HEIGHT=137\n",
    "IMG_WIDTH=236\n",
    "EPCOHS=10\n",
    "TRAIN_BATCH_SIZE=256\n",
    "TEST_BATCH_SIZE=8\n",
    "MODEL_MEAN=(0.485,0.456,0.406)\n",
    "MODEL_STD=(0.229, 0.224, 0.225)\n",
    "BASE_MODEL=\"resnet34\"\n",
    "\n",
    "TRAINING_FOLDS=(0,1,2,3)\n",
    "VALIDATION_FOLDS=(4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VALIDATION_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL_DISPATCHER[BASE_MODEL](pretrained=True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "train_dataset = BengaliDatasetTrain(\n",
    "    folds=TRAINING_FOLDS,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    mean=MODEL_MEAN,\n",
    "    std=MODEL_STD\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_dataset = BengaliDatasetTrain(\n",
    "    folds=VALIDATION_FOLDS,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    mean=MODEL_MEAN,\n",
    "    std=MODEL_STD\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",patience=5,factor=0.3,verbose=True)\n",
    "\n",
    "if torch.cuda.device_count()>1:\n",
    "    model = nn.DataParallel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs,targets):\n",
    "    o1,o2,o3 = outputs\n",
    "    t1,t2,t3 = targets\n",
    "    \n",
    "    l1 = nn.CrossEntropyLoss()(o1,t1)\n",
    "    l2 = nn.CrossEntropyLoss()(o2,t2)\n",
    "    l3 = nn.CrossEntropyLoss()(o3,t3)\n",
    "    return (l1+l2+l3) / 3\n",
    "    \n",
    "    \n",
    "def train(dataset,data_loader, model, optimizer,epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for bi, d in tqdm(enumerate(data_loader), total=int((len(dataset)/data_loader.batch_size))):\n",
    "        image = d[\"image\"]\n",
    "        grapheme_root = d[\"grapheme_root\"]\n",
    "        vowel_diacritic = d[\"vowel_diacritic\"]\n",
    "        consonant_diacritic = d[\"consonant_diacritic\"]\n",
    "        \n",
    "        image = image.to(DEVICE,dtype=torch.float)\n",
    "        grapheme_root = grapheme_root.to(DEVICE,dtype=torch.long)\n",
    "        vowel_diacritic = vowel_diacritic.to(DEVICE,dtype=torch.long)\n",
    "        consonant_diacritic = consonant_diacritic.to(DEVICE,dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        targets = (grapheme_root,vowel_diacritic,consonant_diacritic)\n",
    "        loss = loss_fn(outputs,targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if bi % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, bi * len(d), len(data_loader.dataset),\n",
    "                       100. * bi / len(data_loader), loss.item()))\n",
    "    # Record loss into the writer\n",
    "    writer.add_scalar('Train/Loss', loss.item(), epoch)\n",
    "    writer.flush()\n",
    "        \n",
    "def evaluate(dataset,data_loader, model):\n",
    "    model.eval()\n",
    "    final_loss=0\n",
    "    counter=0\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=int((len(dataset)/data_loader.batch_size))):\n",
    "        counter = counter+1\n",
    "        image = d[\"image\"]\n",
    "        grapheme_root = d[\"grapheme_root\"]\n",
    "        vowel_diacritic = d[\"vowel_diacritic\"]\n",
    "        consonant_diacritic = d[\"consonant_diacritic\"]\n",
    "        \n",
    "        image = image.to(DEVICE,dtype=torch.float)\n",
    "        grapheme_root = grapheme_root.to(DEVICE,dtype=torch.long)\n",
    "        vowel_diacritic = vowel_diacritic.to(DEVICE,dtype=torch.long)\n",
    "        consonant_diacritic = consonant_diacritic.to(DEVICE,dtype=torch.long)\n",
    "        \n",
    "        outputs = model(image)\n",
    "        targets = (grapheme_root,vowel_diacritic,consonant_diacritic)\n",
    "        loss = loss_fn(outputs,targets)\n",
    "        final_loss +=loss\n",
    "    return final_loss / counter\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPCOHS):\n",
    "    train(train_dataset,train_loader,model,optimizer,epoch)\n",
    "    with torch.no_grad():\n",
    "        val_score = evaluate(valid_dataset,valid_loader,model)\n",
    "        scheduler.step(val_score)\n",
    "    torch.save(model.state_dict(),f\"{BASE_MODEL}_fold{VALIDATION_FOLDS[0]}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FOLDS=(0,1,2,4)\n",
    "VALIDATION_FOLDS=(3,)\n",
    "\n",
    "for epoch in range(EPCOHS):\n",
    "    train(train_dataset,train_loader,model,optimizer,epoch)\n",
    "    with torch.no_grad():\n",
    "        val_score = evaluate(valid_dataset,valid_loader,model)\n",
    "        scheduler.step(val_score)\n",
    "    torch.save(model.state_dict(),f\"{BASE_MODEL}_fold{VALIDATION_FOLDS[0]}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FOLDS=(0,1,4,3)\n",
    "VALIDATION_FOLDS=(2,)\n",
    "\n",
    "for epoch in range(EPCOHS):\n",
    "    train(train_dataset,train_loader,model,optimizer)\n",
    "    val_score = evaluate(valid_dataset,valid_loader,model)\n",
    "    scheduler.step(val_score)\n",
    "    torch.save(model.state_dict(),f\"{BASE_MODEL}_fold{VALIDATION_FOLDS[0]}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FOLDS=(0,4,2,3)\n",
    "VALIDATION_FOLDS=(1,)\n",
    "\n",
    "for epoch in range(EPCOHS):\n",
    "    train(train_dataset,train_loader,model,optimizer)\n",
    "    val_score = evaluate(valid_dataset,valid_loader,model)\n",
    "    scheduler.step(val_score)\n",
    "    torch.save(model.state_dict(),f\"{BASE_MODEL}_fold{VALIDATION_FOLDS[0]}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FOLDS=(4,1,2,3)\n",
    "VALIDATION_FOLDS=(0,)\n",
    "\n",
    "for epoch in range(EPCOHS):\n",
    "    train(train_dataset,train_loader,model,optimizer)\n",
    "    val_score = evaluate(valid_dataset,valid_loader,model)\n",
    "    scheduler.step(val_score)\n",
    "    torch.save(model.state_dict(),f\"{BASE_MODEL}_fold{VALIDATION_FOLDS[0]}.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
